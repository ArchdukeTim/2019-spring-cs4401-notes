---
title:  "Lecture Notes: DieHard"
date:   2019-03-28 09:00:00
categories: notes lecture
layout: post
author: Robert Walls
---

This week's paper, ["DieHard: Probablistic Memory Safety for Unsafe
Languages][diehard],  describes a new memory allocator that protects the heap
from the following  memory management errors and, in many cases, will allow the
program to continue executing in the presence of bugs:
 - dangling pointer: a live pointer that points to freed memory.
 - buffer overflows (heap-based): like the stack-based variety, but on the
   heap;
 - heap metadata overwrites: like we saw in last lecture;
 - unitialized reads: reading values from newly-allocated (but unitialized)
   memory or un-allocated memory; 
 - invalid frees: passing an illegal address to free;
 - double frees: repeated calls to free of objects that have already been
   freed.  


[diehard]:/papers/diehard.pdf


*What is the key insight of the paper?* if an allocator has infinite memory to
work with, the allocator can avoid all of the above errors (except unintialized
reads) by placing each object infinitely far apart from all other objects and
never deallocating memory used for an object. Even if an allocator has limited
memory, a buffer overlow isn't a problem as long as the overwritten memory
isn't used for anything else. 


*What is DieHard?* DieHard is (in part) a memory allocator that approximates
inifinite memory. That is, it sacrifices space (and spacial locality, oh those
TLB misses) to spread out objects. It does some clever things with small
objects and fixed sized slots to reduce external fragmentation, the size of
metadata, and to make runtime checks efficient.  Further, their  allocation
scheme allows for computable lower-bounds on the probability of certain errors.
Importantly, DieHard also allows a program to continue executing even in the
presence of some bugs. Most memory-protection techniques will just detect the
error and crash. Some of DieHard's protections are probabilistic and some are
deterministic.

*What is memory safety?*  The DieHard paper defines it as:

> We define a program as being fully memory safe if it satisfies the following
> criteria: it never reads uninitialized memory, performs no illegal operations
> on the heap (no invalid/double frees), and does not access freed memory (no
> dangling pointer errors).

Hmmm, this definition departs quite a bit from what we will see in some other
papers. For instance, their definition only seems to apply to the heap.
Further, it does not have any notion of bounds, i.e., the size of a buffer.


*What bug classes does DieHard eliminate?* This only applies to small objects
(<16k). Double and invalid frees: Because it checks if memory is allocated via
the bitmap if the supplied address is at the right offset. Heapdata overwites
(caused by buffer overflows) because the metadata is separate from the actual
data.

*What bug classes does DieHard offer a probablistic defense?* DieHard offers
probablistic defense for:
 - dangling pointers: because allocations are randomly spread out. Dangling
   pointers are only a problem when the memory is reallocated. With DieHard
you can calculate the probability any given allocation will fall in that slot.  
 - buffer overflow: it is okay to have a buffer overflow as long as it doesn't
   overwrite allocated memory (and that memory isn't later allocated).  


*How does DieHard protect against unitialized reads?* To protect against
unitialized reads, the idea is create separate copies of the program (e.g.,
replicas) and to initialize memory to random values. Reads from  unallocated
memory will return random values and the value returned from one replica will
be  different from the value returned by another replica with high probability. 

While it is nice that replicas can be used to detect unintialized reads, the
current implementation is costly and only works for UNIX-style stdio programs.
In particular, each replica is a separate process and Diehard uses some strange
I/O redirection.  


*Why use a set of differently-sized regions (for small objects)?* DieHard uses
a set of differently-sized regions for efficiency:
 - it reduces external fragmentation (but you get more internal fragmentation)
 - using powers of two for those sizes to allow for fast allocation because you
   can use bit-shifting instead of expensive division and modulus operations. 
 - it makes it easy to separate the metadata (and the metadata is a simple
   bitmap), i.e., Diehard only needs to store a single bit of metadata
(alloated or not allocated). You don't need the length, because the address
will tell you the max length and you have random initialization that means
overreads don't matter. 


What's up with the "large objects" and guard pages? A large object is one that
is greater than 16K.


*Why is it important that the number of objects in a region does not exceed a
threshold of 1/M?* 
 - It bounds the time it takes to find an open slot in the region.


**Looking toward the future:** In DieHard, Heap meta-data is placed randomly in
memory and protected by guard pages. Many of the defenses we will discuss this
term will rely on  this idea of storing security-critical information/metadata
in a protected memory region.

